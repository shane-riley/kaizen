---
title: "ml Homework: 10"
subtitle: "Assigned April 7, 2022; Due: April 14, 2022"
author: "Shane Riley"
date: "Submission time: April 14, 2022 at 11:00PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators

 

## Overview

This assignment focuses on the architecture of single hidden layer feedforward neural networks. You will practice calculating hidden units and neural network responses for a regression task. You will gain experience understanding the interaction between the hidden unit and output layer parameters. You will fit a regression neural network to data by minimizing the sum of squared errors (SSE), and tune the number of hidden units via a hold-out test set. Lastly, you will use `caret` to manage the resampling and tuning of a neural network for you.  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

You are allowed to add as many code chunks as you see fit to answer the questions.  

## Load packages

This assignment will use packages from the `tidyverse` suite.  

```{r, load_packages}
library(tidyverse)
```

This assignment also uses the `scale_color_colorblind()` function from the `ggthemes` package. If you do not have `ggthemes` already installed please type `install.packages("ggthemes")` into the `R` console, or use the RStudio package installer GUI. You only need to run that command **ONCE**. Once the package is installed, you do **not** need to run the command again.  

## Problem 01

In lecture we discussed that neural networks are just matrix multiplications. Hidden units are essentially transformed linear models. The output layer is a linear basis function model with the hidden units acting as the basis functions. In this problem, you will work through the various matrix calculations. Neural networks have already been fit to a data set and the the neural network parameters (weights and biases) are provided to you.  

As practice, you will work with a noise-free toy problem. The response, $f$, is simply equal to $\mathrm{sin}\left(x\right)$. You will practice neural network calculations to approximate that functional relationship.  

The toy data set is loaded for you in the code chunk below and a glimpse is printed to screen. To distinguish between noisy observations (that we typically work with) the response is named `f` in the data set. The input is named `x` as in previous homework assignments.  

```{r, read_prob_01_data}
url_01 <- 'https://redacted'

prob_01_df <- readr::read_csv(url_01, col_names = TRUE)

prob_01_df %>% glimpse()
```

### 1a)

**Plot `f` with respect to `x` with `ggplot()`. Since the response is noise-free, use `geom_line()` and `geom_point()` geoms to make "connect the dots".**  

#### SOLUTION

```{r, solution_01a}
###
prob_01_df %>% ggplot(mapping=aes(y=f,x=x)) +
  geom_line() +
  geom_point() + 
  theme_bw()
```

### 1b)

A set of neural network parameters are downloaded in the code chunk below. The parameters are stored in a list consisting of two elements. The first element named, `beta_matrix` is a matrix containing the $\beta$-parameters associated with the hidden units. The second, `alpha_vector`, is a "regular vector" containing the output layer parameters. The list is printed to screen for you.  

```{r, read_prob_01_params_a}
url_load_dir <- 'https://redacted'

url_load_01_2a <- paste(paste(url_load_dir, "hw_10_prob_01_params_2a.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_01_2a <- readr::read_rds( file = url_load_01_2a )

params_01_2a
```

The `$beta_matrix` is constructed such that each column corresponds to a separate *hidden unit* within the hidden layer. The particular neural network model associated with the `params_01_2a` parameters therefore has 2 hidden units.  

**Why does the `$beta_matrix` contain 2 rows and why does `$alpha_vector` consist of 3 elements? What does each $\beta$ parameter correspond to relative to the hidden units?**  

#### SOLUTION

The `$beta_matrix` corresponds with the weights and the biases going into the hidden layer
(two connections to the input x and two intercepts),
and the `$alpha_vector` corresponds with the weights and biases going into the output layer
(one connection from each hidden neuron and one intercept).

### 1c)

The hidden units consist of two calculations. The first calculates the "linear predictors" based on the inputs and hidden unit parameters. The second is a non-linear transformation of the "linear predictors". We discussed in lecture that there are many possible non-linear transformation functions to use, but the logistic function is a popular choice. Assume the inputs are stored in a design matrix $\mathbf{X}$, which includes a column of 1s, and the hidden unit parameters are stored in a matrix $\mathbf{B}$.  

**Write out the expressions for the linear predictor matrix $\mathbf{A}$ and the non-linear hidden unit values $\mathbf{H}$ assuming a logistic (inverse logit) function is used.**  

#### SOLUTION

First, the hidden values are determined using $\mathbf{B}$ and $\mathbf{X}$, combined
with a logistic function to transform the output. The operation is shown elementwise:

$$
\mathbf{H}_{n} \left( \mathbf{X} \right) = \textrm{Logistic} \left( B_{0,n} + \sum_{d=1}^{D}x_{n,d}B_{d,n}\right)
$$
or,

$$
\mathbf{A} = \mathbf{X} \mathbf{B}
$$
$$
\mathbf{H} = \textrm{Logistic} \left( \mathbf{A} \right)
$$


### 1d)

You will now define a function which calculates the hidden unit "linear predictor" and non-linear values. The function is named `calc_hidden_units()` and it has three input arguments. The first, `X`, is the input design matrix, the second, `B`, is the hidden unit parameter matrix, and the third is the non-linear transformation function. The transformation function argument is named `g` to be consistent with the lecture notation of $g\left( \cdot \right)$.  

This function is general and therefore allows passing in an arbitrary non-linear transformation function.  

**Complete the code chunk below. Calculate the linear predictor matrix `A` and the non-linear transformed hidden unit matrix `H`. The results are returned in a list for you.**  

#### SOLUTION

```{r, solution_01d, eval=TRUE}
calc_hidden_units <- function(X, B, g)
{
  ### your code
  A = X %*% B
  H = g(A)
  
  ### book keeping
  return(list(A = A, H = H))
}
```

### 1e)

You will calculate the hidden units for the `prob_01_df` dataset and the `params_01_2a` parameters. The code chunk below provides a function which visualizes the hidden units with respect to a single input, `x`. You will use this function to interpret the behavior of the hidden units. `viz_hidden_trend_wrt_x()` accepts three input arguments. The first, `v_mat`, is a matrix of hidden unit values. The second, `x_df`, is a `data.frame` which must contain a column named `x`. The third, `trend_type`, is a character string containing the "type" of hidden unit values contained in the `v_mat` matrix. The `trend_type` variable is assigned to the legend title. The `trend_type` argument is used to state whether the resulting figure is plotting the hidden unit "linear predictors" or the non-linear hidden unit values.  


```{r, define_hidden_viz_func}
viz_hidden_trend_wrt_x <- function(v_mat, x_df, trend_type)
{
  x_df %>% 
    select(all_of(c("x"))) %>% 
    tibble::rowid_to_column("obs_id") %>% 
    left_join(v_mat %>% as.data.frame() %>% 
                tibble::as_tibble() %>% 
                purrr::set_names(sprintf("h_%02d", 1:ncol(v_mat))) %>% 
                tibble::rowid_to_column("obs_id"),
              by = "obs_id") %>% 
    pivot_longer(!c("obs_id", "x")) %>% 
    ggplot(mapping = aes(x = x, y = value)) +
    geom_vline(xintercept = 0, color = 'grey50') +
    geom_line(mapping = aes(color = name, group = name),
              size = 1.15) +
    ggthemes::scale_color_colorblind(trend_type) +
    theme_bw() +
    theme(legend.position = "top")
}
```


**IMPORTANT**: In this problem you will use the **logistic** function as the non-linear (activation) function.  

**Complete the two code chunks below. You must create the design matrix `X01`. You must call the `calc_hidden_units()` function by passing in the appropriate arguments and storing the result to `nnet_hidden_2a`. Two calls to `viz_hidden_trend()` are started for you. Complete the calls by assigning the correct element from `nnet_hidden_2a`, `A` or `H`, as the first argument to the `viz_hidden_trend()` call.**  

*HINT*: The third argument to `viz_hidden_trend_wrt_x()` tells you to whether to assign the "linear predictors" or the non-linear hidden unit values.  

*HINT*: Use the `$` operator to access the elements from the list `nnet_hidden_2a`.  

*HINT*: What function can we use for the logistic function?  


#### SOLUTION

```{r, solution_01e_a, eval=TRUE}
X01 <- model.matrix( ~ x, data=prob_01_df)

nnet_hidden_2a <- calc_hidden_units(X01, params_01_2a$beta_matrix, boot::inv.logit)
```

Visualize the hidden unit trends below.  

```{r, solution_01e_b, eval=TRUE}
viz_hidden_trend_wrt_x(nnet_hidden_2a$A,
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_2a$H,
                       prob_01_df,
                       "hidden non-linear units")
```

### 1f)

**Describe the trends of the hidden linear predictors relative to the $\beta$ parameter values displayed in Problem 1b).**  

#### SOLUTION

The straight lines drawn by the linear predictors w.r.t. x match the lines drawn
by the parameters in the $\beta$ matrix (where each column contains an intercept
and a slope). `h_01` maps to the first column, and `h_02` maps to the second
column.

### 1g)

The neural network response, $f$, is calculated as a linear combination of the non-linear hidden unit values in the output layer. Assume the output layer consists of an intercept (bias) $\alpha_{0}$ and a column vector of slopes (weights) $\boldsymbol{\alpha}$.  

**Write out the expression for the response vector $\mathbf{f}$ given the output layer parameters, $\alpha_{0}$, $\boldsymbol{\alpha}$, and the matrix of non-linear hidden unit values $\mathbf{H}$.**  

#### SOLUTION

The weights and bias from the $\alpha$ vector are decoupled to create the following
eqaution for $\mathbf{f}$:

$$
\mathbf{f} = \alpha_0 + \mathbf{H} \mathbf{\alpha}
$$

### 1h)

As with the hidden units, you will use a function to calculate the neural network response. The code chunk below defines the `calc_nnet_response()` function. It accepts two arguments. The first, `H`, is the matrix of non-linear hidden unit values and the second, `a`, is the "regular vector" of output layer parameters. Note that `a` contains the intercept (the bias) and the slopes (the weights).  

**Complete the code chunk below. You must separate the `a` vector into the intercept (bias) and the slopes (weights). Store the intercept as the `a_0` variable and the slopes as the `a_w` regular vector. You must then convert the `a_w` regular vector into a column vector `a_col`. Finally, you must calculate the response, `f`. The response is returned as a vector for you.**  

#### SOLUTION

```{r, solution_01h, eval=TRUE}
calc_nnet_response <- function(H, a)
{
  ### separate the vector into bias and weights
  a0 <- a[1]
  a_w <- a[2:length(a)]
  
  # convert the weights to a column vector
  a_col <- matrix(a_w)
  
  # calculate the response (the output layer)
  f <- a0 + H %*% a_col
  
  as.vector(f)
}
```

### 1i)

With all the calculations completed, let's now compare the neural network response to the true sine wave.  

**Complete the code chunk below by assigning the correct the arguments to the `calc_nnet_response()` function. The rest of the code, which generates the figure, is completed for you.**  

**How would you describe the neural network's fit? Which portions are approximated well?**  

#### SOLUTION

```{r, solution_01i, eval=TRUE}
prob_01_df %>% 
  mutate(nnet_f = calc_nnet_response(nnet_hidden_2a$H, params_01_2a$alpha_vector)) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = nnet_f),
            color = "black", size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", shape = 1, size = 3.5) +
  labs(y = "f") +
  theme_bw()
```

The model is able to approximate the curve decently well. Considering the
fact that there are 7 degrees of freedom (4 from $\mathbf{\beta}$ and 3 from $\mathbf{\alpha}$),
however, the model does not approximate the curve as well as a 7th order polynomial
could.

## Problem 02

You will now repeat your calculations from Problem 1. You will first try out a different set of parameters for a two hidden unit neural network. Then you consider a neural network with 5 hidden units.  

### 2a)

The code chunk below reads in a new set of neural network parameters. The format is consistent with that from Problem 1, in that the hidden unit parameters are contained within the element `$beta_matrix` and the output layer parameters are stored in `$alpha_vector`. The parameters are printed to the screen below.  

```{r, read_in_prob_02a_params_b}
url_load_01_2b <- paste(paste(url_load_dir, "hw_10_prob_01_params_2b.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_01_2b <- readr::read_rds( url_load_01_2b )

params_01_2b
```

You will use these new parameters to calculate the hidden unit "linear predictors" and the non-linear hidden unit values for the two hidden unit neural network.  

**Complete the code chunk below. Call the `calc_hidden_units()` function with the appropriate arguments and assign the result to the `nnet_hidden_2b` object. Then assign the correct arguments to the `viz_hidden_trend()` functions calls. Are the trends of the hidden unit values (linear and non-linear) consistent with the trends from Problem 1? If not, what would be causing the change? Based on the figures generated in the code chunk below, do you think the resulting neural network will be different from that in Problem 1?**  

#### SOLUTION

```{r, solution_02, eval=TRUE}
nnet_hidden_2b <- calc_hidden_units(X01, params_01_2b$beta_matrix, boot::inv.logit)

viz_hidden_trend_wrt_x(nnet_hidden_2b$A, 
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_2b$H, 
                       prob_01_df,
                       "hidden non-linear units")
```

The figures are slightly different for the hidden parameters, so it is expected
that the predictions will perform differently than the first model.

### 2b)

Let's now compare the responses associated with the two sets of parameters.  

**Complete the first code chunk below by assigning the correct arguments to the two `calc_nnet_response()` calls. The first call is associated with the parameters from Problem 1, with the result stored to the `nnet_fa` variable. The second call is associated with the new set of parameters, and the result is stored to the `nnet_fb` variable.**  

**The second code chunk is completed for you. It plots the two responses together with the true sine wave output. Based on the figure below, how do the two different neural network models compare?**  

#### SOLUTION

```{r, solution_02b_a, eval=TRUE}
results_2hidden <- prob_01_df %>% 
  mutate(nnet_fa = calc_nnet_response(nnet_hidden_2a$H, params_01_2a$alpha_vector),
         nnet_fb = calc_nnet_response(nnet_hidden_2b$H, params_01_2b$alpha_vector))
```

Now visualize the neural network predictions and compare to the true sine wave.  

```{r, solution_02b_b, eval=TRUE}
results_2hidden %>% 
  pivot_longer(!c("x", "f")) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = name,
                          linetype = name,
                          color = name),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  ggthemes::scale_color_colorblind("Model") +
  scale_linetype_discrete("Model") +
  labs(y = "f") +
  theme_bw()
```

It is clear based on the figure that the second neural net vastly outperforms
the first one in approximating the original curve.

### 2c)

**Based on your results, can you "explain" or interpret the neural network behavior by ONLY examining the slopes (weights) acting on the inputs?**  

#### SOLUTION

It is not trivial to explain the vast difference in output behavior by simply
examining the $\mathbf{\beta}$ matrix. This is in large part due to the nonlinear
logistic function applied to the hidden units, which makes the influence of each
weight and bias much less clear.

### 2d)

Let's now perform the same type of calculations, but on a neural network with 5 hidden units instead of 2. As before, you will compare two sets of parameters for the 5 hidden unit model. The code chunk below reads in the two different sets of neural network parameters. The format is consistent with that from Problem 1, except now there are more parameters since there are more hidden units.  


```{r, read_in_prob_2_5_hidden_params}
url_load_01_5a <- paste(paste(url_load_dir, "hw_10_prob_01_params_5a.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_02_5a <- readr::read_rds( url_load_01_5a )

url_load_01_5b <- paste(paste(url_load_dir, "hw_10_prob_01_params_5b.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_02_5b <- readr::read_rds( url_load_01_5b )
```

The first set neural network parameters, `params_02_5a`, are displayed for you below to show the difference in structure with the 2 hidden unit neural networks you worked with previously.  

```{r, show_prob_2_5_hidden_params_a}
params_02_5a
```


**Calculate the hidden unit "linear predictors" and non-linear values for the two different 5 hidden unit models. Then complete the calls to the `viz_hidden_trend_wrt_x()` function to plot the hidden unit trends with respect to the input.**  

**Describe the trends of the non-linear values for the fifth hidden unit, `h_05`, relative to its "linear predictor" values. Discuss the differences between the two models (sets of parameters).**  

#### SOLUTION

Calculate the hidden unit "linear predictors" and non-linear hidden unit values.  

```{r, solution_02d_a, eval=TRUE}
nnet_hidden_5a <- calc_hidden_units(X01, params_02_5a$beta_matrix, boot::inv.logit)

nnet_hidden_5b <- calc_hidden_units(X01, params_02_5b$beta_matrix, boot::inv.logit)
```

Visualize the behavior of the hidden units associated with the `params_02_5a` parameters.  

```{r, solution_02d_b, eval=TRUE}
viz_hidden_trend_wrt_x(nnet_hidden_5a$A, 
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_5a$H, 
                       prob_01_df,
                       "hidden non-linear units")
```

Visualize the behavior of the hidden units associated with the `params_02_5b` parameters.  

```{r, solution_02d_c, eval=TRUE}
viz_hidden_trend_wrt_x(nnet_hidden_5b$A, 
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_5b$H, 
                       prob_01_df,
                       "hidden non-linear units")
```
The easiest differences to discern between the two sets of linear predictors and
hidden units is the intercepts and the maximum magnitudes. The intercepts for all
of the linear predictors of `5b` are clustered together, while in `5a` they are more
spread out. Additionally, the highest linear predictor magnitudes of `5b` do not exceed 6, but go
over 20 in `5a`. These differences are reflected in the logistic plots of the hidden
units.

### 2e)

Let's now calculate the neural network response for both with 5 hidden units.  

**Complete the first code chunk below by assigning the arguments correctly to the two `calc_nnet_response()` function calls. The first call is intended for the model associated with `params_02_5a` while the second call is intended for the model associated with `params_02_5b`.**  

**The second code chunk is completed for you. How do the two models compare to each other and to the true sine wave?**  

#### SOLUTION

```{r, solution_02e_a, eval=TRUE}
results_5hidden <- prob_01_df %>% 
  mutate(nnet_fa = calc_nnet_response(nnet_hidden_5a$H, params_02_5a$alpha_vector),
         nnet_fb = calc_nnet_response(nnet_hidden_5b$H, params_02_5b$alpha_vector))
```

Visualize the two neural network model predictions and compare with the true sine wave.  

```{r, solution_02e_b, eval=TRUE}
results_5hidden %>% 
  pivot_longer(!c("x", "f")) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = name,
                          linetype = name,
                          color = name),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  ggthemes::scale_color_colorblind("Model") +
  scale_linetype_discrete("Model") +
  labs(y = "f") +
  theme_bw()
```

### 2f)

The code chunk below is completed for you. The 2 hidden unit and 5 hidden unit model predictions are compared side by side.  

```{r, solution_02f_a, eval=TRUE}
results_2hidden %>% 
  mutate(num_hidden = 2) %>% 
  bind_rows(results_5hidden %>% 
              mutate(num_hidden = 5)) %>% 
  pivot_longer(!c("x", "f", "num_hidden")) %>% 
  tidyr::separate(name,
                  c("nnet_word", "fparams"),
                  sep = "_") %>% 
  tidyr::separate(fparams,
                  c("fltr", "paramset"),
                  sep = 1) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = interaction(paramset,
                                              num_hidden),
                          linetype = paramset,
                          color = paramset),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  facet_grid( ~ num_hidden, labeller = "label_both") +
  ggthemes::scale_color_colorblind("Params") +
  scale_linetype_discrete("Params") +
  labs(y = "f") +
  theme_bw()
```

**Based on the figure above, which model, the 2 hidden units or 5 hidden units, performs better? What controls complexity within a neural network model and how could you go about "tuning" that complexity?**  

#### SOLUTION

It is clear from the figures that the models with 5 hidden units perform much
better than the models with only 2. It is also shown that a model with more hidden
units will be more complex than a model with fewer. This complexity can also be tuned
by 'dropping out' certain hidden nodes while training.

### 2g)

The toy data within Problem 1 and 2 comes from a simple sine wave:  

$$ 
f\left(x\right) = \mathrm{sin}\left(x\right)
$$

Let's see if a linear model, using the correct `sin()` basis can correctly identify that the "slope" acting on `sin(x)` is 1.  

**Use the `lm()` function to a fit a linear model for the response `f` and the sine of the input, `sin(x)`. Use the `prob_01_df` data set as the `data` argument to the `lm()` call. Assign the result to the `lm_sine_mod` object.**  

**Print the `summary()` of the `lm()` call to the screen. What is the estimate for the slope associated with the `sin(x)`?**  

#### SOLUTION

```{r, solution_02g, eval=TRUE}
lm_sine_mod <- lm(f ~ sin(x),
                  data = prob_01_df)

### print the summary to the screen
lm_sine_mod %>% summary()
```

What is the estimate to the slope acting on `sin(x)`?

The estimated slope acting on `sin(x)` is exactly 1, with virtually no uncertainty.

### 2h)

**How many unknown parameters were there in the linear model with the sine wave basis function? How many unknown parameters existed in the neural network with 5 hidden units?**  

#### SOLUTION

The linear model had only one unknown parameter (the slope acting on `sin(x)`),
while the 5 hidden unit network had 16 unknown parameters (10 from $\mathbf{\beta}$
and 5 from $\mathbf{\alpha}$).

### 2i)

**It was really simple to fit the linear model. Why would we want to use a neural network when we can build the exact model in this application using `lm()`?**  

#### SOLUTION

In this exact application, there is virtually no reason to use a neural network
since a perfect linear basis is provided. Using that basis, a fit can be found
with one degree of freedom and is easy to explain. However, if there is no linear basis
or one that is hard to find for a curve, than a neural network becomes more able to outperform
a linear basis model.

## Problem 03

In the previous problems, you focused on the predictions of a neural network. You will now work through fitting neural networks, on a slightly more realistic example. The code chunk below reads a data set consisting of 3 continuous inputs, `x1`, `x2`, and `x3`, and a continuous response, `y`. A glimpse of the data set is displayed to the screen for you.  

```{r, read_prob_03_data}
url_03_train <- 'https://redacted'

prob_03_df <- readr::read_csv( url_03_train, col_names = TRUE)

prob_03_df %>% glimpse()
```

### 3a)

The wide-format data set is converted into a long-format data set for you in the code chunk below. The glimpse displayed to the screen shows that the inputs have been "stacked" or "gathered" together into a column `name` with their values given in the `value` column.  

```{r, make_03_longformat}
prob_03_lf <- prob_03_df %>% 
  tibble::rowid_to_column("obs_id") %>% 
  pivot_longer(!c("obs_id", "y"))

prob_03_lf %>% glimpse()
```

**Plot the noisy response, `y`, with respect to each input using the long-format data set. Using the `geom_point()` geom and create separate facets (subplots) for each input using `facet_wrap()`.**  

**What trends do you see in the scatter plots?**  

#### SOLUTION

```{r, solution_03a}
###
prob_03_lf %>% ggplot(mapping=aes(y=y,x=value)) +
  facet_wrap(vars(prob_03_lf$name)) +
  geom_point()
  
```

There appears to be a slight positive trend between `y` and `x2`, as well as a
negative trend between `y` and `x3`.

### 3b)

In the previous problems you used a logistic function as the non-linear function associated with each hidden unit. However, there are many different functions that could be used. To get exposure working with a different "activation" function, you will use the hyperbolic tangent function for this problem. Let's first get an idea about how the hyperbolic tangent compares with the logistic function.  

**Complete the code chunk below by setting `x` to be 101 equally spaced points between -5.5 and 5.5. Calculate the the logistic function of `x` and assign the result to `logistic_result`. Calculate the hyperbolic tangent of `x` and assign the result to `tanh_result`. The result of the code chunk is completed for you. It visualizes the non-linear transformations with respect to the `x` variable.**  

**Is the hyperbolic tangent function similiar to the logistic function? In what ways are the two different?**  

*HINT*: The hyperbolic tangent function in `R` is `tanh()`.  

```{r, solution_03b, eval=TRUE}
tibble::tibble(
  x = seq(-5.5, 5.5, length.out=101)
) %>% 
  mutate(logistic_result = boot::inv.logit(x),
         tanh_result = tanh(x)) %>% 
  # rest of the code here is completed for you
  pivot_longer(!c("x")) %>% 
  ggplot(mapping = aes(x = x, y = value)) + 
  geom_hline(yintercept = c(-1, 0, 1),
             color = 'grey50', linetype = 'dashed') +
  geom_line(mapping = aes(y = value,
                          color = name,
                          linetype = name),
            size = 1.15) +
  ggthemes::scale_color_calc("") +
  scale_linetype_discrete("") +
  theme_bw() +
  theme(legend.position = "top") 
```

Both functions squeeze an input between a lower and an upper bound. The `logistic`
function squeezes this input into $(0,1)$ with $\textrm{Logistic}(0) = 0.5$, and
the `tanh` function into $(-1,1)$ with $\textrm{tanh}(0) = 0$. Both functions are
also increasing monotonically.


### 3c)

You will fit the neural network by minimizing the sum of squared errors (SSE). Thus, we will work with a non-probabilistic setting, even though we derived the linear and generalized linear model fitting with likelihoods and priors. Remember that minimizing the SSE is analogous to maximizing a Gaussian log-likelihood!  

**Write the expression for the SSE using the observed response $y$ and the neural network response $f$. You may write the SSE in either the summation or matrix/vector notation. If you use a summation notation use the subscript $n$ to denote a single observation. If you use matrix/vector notation denote the response vector as $\mathbf{y}$.**  

#### SOLUTION

In summation form:

$$
\textrm{SSE} = \sum_{n=1}^{N}\left( \left( y_n - f_n \right)^2\right)
$$


### 3d)

You will now program the error function we wish to minimize in the style of the log-posterior functions from earlier homework assignments. You will name your function `my_neuralnet_sse()`. It will consist of two input arguments, a vector of parameters to learn and a list of required information. Before defining the function, you will create the list of required information, which is started for you in the code chunk below. Notice that the structure is similar to the lists of information created for the generalized linear models. However, two pieces of information not associated with GLMs are required for the neural network. The variable `$num_hidden` specifies the number of hidden units and the variable `$transform_hidden` stores the non-linear transformation function to apply to each hidden unit.  

You will start out with a small neural network consisting of 3 hidden units. You will use the hyperbolic tangent as the non-linear transformation function, instead of the logistic function that you worked with in the previous problems. You will therefore need to assign the `tanh()` function correctly to the `$transform_hidden` field in the list. Be careful about the `()` when assigning the function *object*!  

You will need to specify the design matrix for your neural network based on the 3 inputs in the `prob_03_df` data set. Think carefully how the design matrix is structured in a neural network.  

**Complete the list of required information by completing the code chunk below. You must create the design matrix based on the three inputs. Assign the design matrix to the `$design_matrix` variable in the list. Assign the observed responses to the `$yobs` variable in the list. Set the number of hidden units to be 3.**  

**After specifying the `info_three_units` list, calculate the total number of parameters in the single hidden layer neural network with 3 hidden units and assign the result to the `info_three_units$num_params` variable.**  

#### SOLUTION

The code chunk is started for you below.  

```{r, solution_03d, eval=TRUE}
### design matrix
Xmat_03 <- model.matrix(y ~ x1 + x2 + x3, data=prob_03_df)

info_three_units <- list(
  yobs = prob_03_df$y,
  design_matrix = Xmat_03,
  num_hidden = 3,
  transform_hidden = tanh
)

info_three_units$num_params <- (ncol(Xmat_03) * info_three_units$num_hidden) +  # to hidden layer
  (info_three_units$num_hidden + 1)  # from hidden layer
```

The total number of hidden units you calculated in the above code chunk are printed to the screen below.  

```{r, solution_03d_b, eval=TRUE}
info_three_units$num_params
```


### 3e)

You will now define the $SSE$ objective in the `my_neuralnet_sse()` function below. As described previously, the function consists of two input arguments. The first argument, `theta`, contains all of the unknown parameters to learn. The vector is organized with all hidden unit parameters listed before the output layer parameters. The first part of the `my_neuralnet_sse()` function has several portions completed for you. **You are responsible for determining the number of hidden unit parameters (the betas) for each hidden unit.** You should **not** hard code `length_beta_per_unit`. You must then calculate the total number of hidden unit parameters and assign the result to `total_num_betas`. Again you should **not** hard code this number because later on you will try out more hidden units.  

The hidden unit parameters are extracted from the `theta` vector and organized into the `Bmat` matrix with dimensions consistent with the $\mathbf{B}$ described in Problem 01 and 02.  

The output layer parameters are extracted for you and assigned to the `a_all` vector. You must reorganize the output layer parameters by separating the bias, `a0`, and output layer weights, `aw`. The bias should be a scalar quantity and the output layer weights should be a "regular vector".  

You must complete the function by performing the necessary matrix math calculations, transformations, and calculation of the $SSE$. The comments in the function describe what you must complete in each line.  

After completing the function, test that it works using two separate guesses for the unknown parameters. First set all parameters to a value of 0, then set all parameters to a value of -1.25. If your function is specified correctly the $SSE$ should be `683.113` for the guess of all 0's and it should be `2238.39` for the guess -1.25 for all parameters.  

**Complete the `my_neuralnet_sse()` function below and test it's operation with the two guesses specified in the problem statement.**  

#### SOLUTION

The `my_neuralnet_sse()` function is started for you in the code chunk below.  

```{r, solution_03e, eval=TRUE}
my_neuralnet_sse <- function(theta, my_info)
{
  # extract the hidden unit parameters
  X <- my_info$design_matrix
  length_beta_per_unit <- ncol(my_info$design_matrix)  # how many betas are there???????
  total_num_betas <- length_beta_per_unit * my_info$num_hidden  # how many total betas are there???????
    
  beta_vec <- theta[1:total_num_betas]
  
  # reorganize the beta parameters into a matrix
  Bmat <- matrix(beta_vec, nrow = length_beta_per_unit, byrow = FALSE)
  
  # extract the output layer parameters
  a_all <- theta[(total_num_betas + 1):length(theta)]

  # reorganize the output layer parameters by extracting
  # the output layer intercept (the bias)
  a0 <- a_all[1]  # output layer bias??????
  aw <- matrix(a_all[2:length(a_all)]) # output layer weights?????
  
  # calculate the linear predictors associated with
  # each hidden unit
  A <- X %*% Bmat
  
  # pass through the non-linear transformation function
  H <- my_info$transform_hidden(A)
  
  # calculate the response (the output layer)
  f <- a0 + H %*% aw
  
  # calculate the SSE
  sum((my_info$yobs - f)^2)
  
}
```

Test out your `my_neuralnet_sse()` function with values of 0 for all parameters.  

```{r, solution_03e_b}
my_neuralnet_sse(rep(0,16), info_three_units)
```

Test out your `my_neuralnet_sse()` function with values of -1.25 for all parameters.  

```{r, solution_03e_c}
my_neuralnet_sse(rep(-1.25,16), info_three_units)
```

### 3f)

With the objective function completed, it's now time to fit the simple neural network with 3 hidden units. You will use the `optim()` function to perform the optimization, just as in the previous assignments. Since we are focused on finding the estimates at the moment, you will work with `optim()` itself, rather than within the `my_laplace()` wrapper as in previous assignments.  

You will fit two neural networks from two different starting guess values. The first starting guess will be a vector of 0's, and the second guess will be -1.25 for all parameters. Complete the two code chunks below by specifying the initial guesses correctly and completing the remaining input arguments to the `optim()` call. You must set the `gr` argument to `r NULL` so that `optim()` uses finite differences to estimate the gradient vector. Pass in the `info_three_units` list of required information to both `optim()` calls. Specify the `method` argument to be `"BFGS"` to use the quasi-Newton BFGS algorithm. Set the `hessian` argument to be `r FALSE` which forces the Hessian matrix to **NOT** be estimated at the end. We are simply interested in the point estimates at the moment and so we will not be concerned with the curvature of the error surface. The maximum number of iterations is set for you in both `optim()` calls already.  

**Complete both code chunks below in order to fit the three hidden unit neural network with two different starting guesses. Follow the instructions in the problem statement to specify all the arguments to the `optim()` calls.**  

**After fitting, print out the identified optimal parameters contained in the `$par` field of the `optim()` results for both cases. Are the identified optimal parameter values the same between the two starting guesses? Why would the results not be the same?**  

#### SOLUTION

Fit the neural network with the initial guess of 0's for all parameters.  

```{r, solution_03f_a, eval=TRUE}
optim_fit_3_a <- optim(rep(0,16),
                       my_neuralnet_sse,
                       gr = NULL,
                       info_three_units,
                       method = "BFGS",
                       hessian = FALSE,
                       control = list(maxit = 5001))
```


Fit the neural network with the initial guess of -1.25 for all parameters.  

```{r, solution_03f_b, eval=TRUE}
optim_fit_3_b <- optim(rep(-1.25,16),
                       my_neuralnet_sse,
                       gr = NULL,
                       info_three_units,
                       method = "BFGS",
                       hessian = FALSE,
                       control = list(maxit = 5001))
```


Compare the optimized parameter estimates.  

```{r, solution_03f_c}
###
optim_fit_3_a$value
optim_fit_3_b$value
optim_fit_3_a$converge
optim_fit_3_b$converge
optim_fit_3_a$par
optim_fit_3_b$par

```
Since both models converged to different local optima, the expectation that the
loss-space of the inputs is multi-modal is confirmed. This means that the optimal
solution found will depend on the initial guess, unlike with linear models.

### 3g)

Fit the neural network with 3 hidden units again, but this time use 2 randomly generated initial guess values. Use standard normals (mean 0 and standard deviation 1) to generate the initial guesses.  

**Complete the two code chunks below by generating two random initial guess values. Assign the first random initial guess to `init_guess_03_c` and the second random initial guess to `init_guess_03_d`.**  
**Complete the `optim()` calls following the same instructions as the previous question.**  

**Check if the optimized parameter estimates are the same or not.**  

#### SOLUTION

Set the random initial guess values.  

```{r, solution_03g_a, eval=TRUE}
set.seed(412412)
init_guess_03_c <- rnorm(16)
  
set.seed(214214)
init_guess_03_d <- rnorm(16)
```

Run the optimization for the first random initial guess.  

```{r, solution_03g_b, eval=TRUE}
optim_fit_3_c <- optim(init_guess_03_c,
                       my_neuralnet_sse,
                       gr = NULL,
                       info_three_units,
                       method = "BFGS",
                       hessian = FALSE,
                       control = list(maxit = 5001))
```

Run the optimization for the first random initial guess.  

```{r, solution_03g_c, eval=TRUE}
optim_fit_3_d <- optim(init_guess_03_d,
                       my_neuralnet_sse,
                       gr = NULL,
                       info_three_units,
                       method = "BFGS",
                       hessian = FALSE,
                       control = list(maxit = 5001))
```

Compare the parameter estimates.  

```{r, solution_03g_d}
###
optim_fit_3_c$value
optim_fit_3_d$value
optim_fit_3_c$convergence
optim_fit_3_d$convergence
optim_fit_3_c$par
optim_fit_3_d$par
```
In this example, the two randomly-generated inputs reach the same solution of weights,
and therefore the same minimum SSE.

### 3h)

The `optim()` results store the objective function value as the `$value` field in the returned list object.  

**Compare the SSE for the 4 different starting guesses. Which model is better, as viewed by the training set?**  

#### SOLUTION

```{r, solution_03h}
# Compare minima
  which.min(c(optim_fit_3_a$value,
              optim_fit_3_b$value,
              optim_fit_3_c$value,
              optim_fit_3_d$value))


```
The comparison above shows that the fourth trained model
minimizes SSE and is thus the highest-performing solution found. It is worth noting
that the second and third models perform about the same, however.


## Problem 04

You now have the major pieces in place for fitting neural networks! In this problem, we will fit additional neural networks with more hidden units!  

### 4a)

Let's define a function which will generate a random initial guess for the appropriate number of unknown parameters and then execute the `optim()` call. The `train_1layer_nnet_sse()` function has 4 input arguments. The first argument, `num_hidden`, is the number of hidden units in the hidden layer, the second, `transform_func`, is the non-linear transformation (activation) function, the third `X`, is the design matrix, and the fourth, `y`, the response vector.  

**Complete the code chunk below which assembles the list of required information and generates the random initial guess, for an arbitrary number of hidden units in the first hidden layer. Do not set the random seed inside the `train_1layer_nnet_sse()` function. We will set the seed before we fit the models.**  

*HINT*: If your function below is setup correctly you should be able to replicate the previous results if the **SAME** random seed is used. The second code chunk below resets the random seed for you as a confirmation test.  

#### SOLUTION

```{r, solution_04a, eval=TRUE}
train_1layer_nnet_sse <- function(num_hidden, transform_func, X, y)
{
  my_info_list <- list(
    yobs = y,
    design_matrix = X,
    num_hidden = num_hidden,
    transform_hidden = transform_func
  )
  
  my_info_list$num_params <- (ncol(X) * num_hidden) + (num_hidden + 1) # total number of hidden and output layer parameters
  
  # generate random initial guess
  init_guess <- rnorm(my_info_list$num_params)
  
  # call optim to fit the neural network
  optim(init_guess,
         my_neuralnet_sse,
         gr = NULL,
         my_info_list,
         method = "BFGS",
         hessian = FALSE,
         control = list(maxit = 10001))
}
```

As a check fit the 3 hidden unit neural network again with the same random seed as used with `init_guess_03_c`. You should get the same parameters as `optim_fit_3_c`.  

```{r, solution_04a_b, eval=TRUE}
set.seed(412412)
check_optim_fit_3_c <- train_1layer_nnet_sse(3, tanh, Xmat_03, prob_03_df$y)
```

Compare to the previous `optim_fit_3_c` results.  

```{r, solution_04a_c}
### 
check_optim_fit_3_c$par
optim_fit_3_c$par
```
The results are the same.

### 4b)

Let's now fit neural networks with 6, 12, and 24 hidden units instead of 3 hidden units. You will use two different initial guesses for each hidden layer size. The random seeds are set for you to make sure the results are reproducible.  

**Complete the three code chunks below by setting the input arguments to fit 2 pairs of 6, 12, and 24 hidden unit neural networks.**  

#### SOLUTION

```{r, solution_04b, eval=TRUE}
set.seed(412412)
optim_fit_6_a <- train_1layer_nnet_sse(6, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_6_b <- train_1layer_nnet_sse(6, tanh, Xmat_03, prob_03_df$y)
```

```{r, solution_04b_b, eval=TRUE}
set.seed(412412)
optim_fit_12_a <- train_1layer_nnet_sse(12, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_12_b <- train_1layer_nnet_sse(12, tanh, Xmat_03, prob_03_df$y)
```

Please note that fitting the two 24 hidden unit neural networks may take a few minutes.  

```{r, solution_04b_c, eval=TRUE}
set.seed(412412)
optim_fit_24_a <- train_1layer_nnet_sse(24, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_24_b <- train_1layer_nnet_sse(24, tanh, Xmat_03, prob_03_df$y)
```


### 4c)

Compare the training set SSE across all of the models you trained with random initial guesses (including the 3 hidden unit models).  

**Which model was considered the best according to the training set?**  

#### SOLUTION

```{r, solution_04c}
fits <- c(optim_fit_6_a$value,
            optim_fit_6_b$value,
            optim_fit_12_a$value,
            optim_fit_12_b$value,
            optim_fit_24_a$value,
            optim_fit_24_b$value)
fits
which.min(fits)
min(fits)
```

Purely comparing on minimum SSE, the `optim_fit_24_a` model is chosen, with an
SSE of 2.65.

## Problem 05

We know that we should **not** compare models strictly based on the training set (unless we were using an information criterion metric). The code chunk below reads in a hold-out test set for you. This hold out test set will be used to compare the performance across the hidden layer sizes that you have fit so far.  

```{r, read_holdout_set}
url_03_test <- 'https://redacted'

prob_03_test_df <- readr::read_csv( url_03_test, col_names = TRUE)

prob_03_test_df %>% glimpse()
```

### 5a)

You will define a function which makes predictions for you and calculates the Mean Squared Error (MSE) on the test set. The function, `assess_nnet_mse()`, is started for you in the code chunk below. The first argument, `theta`, is a vector of all unknown parameters, the second argument, `num_hidden`, is the number of hidden unit parameters, the third argument, `transform_func`, is the non-linear transformation function, the fourth argument, `X`, is a design matrix, and the fifth argument, `y`, is the response vector.  

**You have worked with the necessary pieces to complete this function several different ways in this assignment. You are free to decide how best to calculate the MSE for a given set of parameters. The only requirement is that `assess_nnet_mse()` should return a scalar number.**  

#### SOLUTION

```{r, solution_05a, eval=TRUE}
assess_nnet_mse <- function(theta, num_hidden, transform_func, X, y)
{
  # (copied from earlier)'
  # extract the hidden unit parameters
  length_beta_per_unit <- ncol(X)
  total_num_betas <- length_beta_per_unit * num_hidden
    
  beta_vec <- theta[1:total_num_betas]
  
  # reorganize the beta parameters into a matrix
  Bmat <- matrix(beta_vec, nrow = length_beta_per_unit, byrow = FALSE)
  
  # extract the output layer parameters
  a_all <- theta[(total_num_betas + 1):length(theta)]

  # reorganize the output layer parameters by extracting
  # the output layer intercept (the bias)
  a0 <- a_all[1]
  aw <- matrix(a_all[2:length(a_all)])
  
  # calculate the linear predictors associated with
  # each hidden unit
  A <- X %*% Bmat
  
  # pass through the non-linear transformation function
  H <- transform_func(A)
  
  # calculate the response (the output layer)
  f <- a0 + H %*% aw
  
  # calculate the MSE
  mean((y - f)^2)
}
```

### 5b)

Before you can calculate the MSE on the hold-out test set, you must create the test design matrix.  

**Create the appropriate test design matrix associated with all 3 inputs, using the `prob_03_test_df` data set, and assign the result to `Xtest_03`.**  

#### SOLUTION

```{r, solution_05b, eval=TRUE}
Xtest_03 <- model.matrix(y ~ x1 + x2 + x3, data=prob_03_test_df)
```

### 5c)

**Calculate the MSE for each of the models trained with random initial guess values. You are free to decide how to execute this task.**  

#### SOLUTION

```{r, solution_05c}

# Pack the models
mses <- c(
  assess_nnet_mse(optim_fit_3_c$par, 3, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_3_d$par, 3, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_6_a$par, 6, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_6_b$par, 6, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_12_a$par, 12, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_12_b$par, 12, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_24_a$par, 24, tanh, Xtest_03, prob_03_test_df$y),
  assess_nnet_mse(optim_fit_24_b$par, 24, tanh, Xtest_03, prob_03_test_df$y)
)
mses

which.min(mses)
min(mses)

```

### 5d)

**Which model is the best as viewed by the hold-out test set performance?**  

#### SOLUTION

According to test MSE, the first model `optim_fit_6_a` trained with 6 hidden units performs the 
best on the test data. It is also worth noting that the optimum from SSE on training data
performs comparatively poorly on an MSE comparison on test data (`optim_fit_24_a`).

## Problem 06

You not only fit neural networks from scratch, but you used a hold-out test set to **tune** the number of hidden units! Doing so required you to directly work with the assumptions of the neural network, learning how to make predictions with the matrix operations, calculate the performance metric, and ultimately assess the potential for overfitting as the complexity increases. Understanding the assumptions and concepts are critical when assessing the behavior and performance of a neural network in a practical application when we use existing functions and packages to fit the neural network for us. In this last problem you will practice using `caret` to manage the training, evaluation, and tuning of a neural network. You will use the `nnet` package to fit the neural network. Please download and install `nnet` if you do not have it already. If you do not install it, `caret` will prompt you to install it and so please check the R console if nothing seems to happen when you use the `caret::train()` function.  

The code chunk below loads the `caret` package for you. You do not need to load `nnet`, the `caret` package will manage that for you.  

```{r, load_caret_package}
library(caret)
```

### 6a)

You must specify the resampling scheme that `caret` will use to train, assess, and tune the model.  

**Specify the resampling scheme to be 5 fold with 3 repeats. Assign the result of the `trainControl()` function to the `my_ctrl` object. Specify the primary performance metric to be `'RMSE'` and assign that to the `my_metric` object.**  

#### SOLUTION

```{r, solution_06a}
### your code here
my_ctrl <- trainControl(
  method="repeatedcv",
  number=5,
  repeats=3
)

my_metric <- "RMSE"
```

### 6b)

In a realistic application, it is always best to first fit linear models before we fit neural networks. The linear models (especially regularized models which include interaction features) serve as interpretable baseline models. However, since this assignment is focused on neural networks we will just fit the neural network. You must train, assess, and tune a neural network using the **default** `caret` tuning grid. In the `caret::train()` function you must use the formula interface to specify the inputs are `x1`, `x2`, and `x3`, while the response is `y`. Assign the `method` argument to `'nnet'` and set the `metric` argument to `my_metric`. You must also instruct `caret` to standardize the features by setting the `preProcess` argument equal to `c('center', 'scale')`. Assign the `trControl` argument to the `my_ctrl` object.  

**Train, assess, and tune the `nnet` neural network with the defined resampling scheme. Assign the result to the `nnet_default` object and print the result to the screen. Which tuning parameter combinations are considered to be the best?**  

**IMPORTANT**: include the argument `trace = FALSE` in the `caret::train()` function call. This will make sure the `nnet` package does NOT print the optimization iteration results to the screen.  

#### SOLUTION

```{r, solution_06b, eval=TRUE}
set.seed(412412)
nnet_default <- train(
  y ~ x1 + x2 + x3,
  method="nnet",
  metric=my_metric,
  preProcess=c("center", "scale"),
  trControl=my_ctrl,
  data=prob_03_df,
  trace="FALSE"
)
nnet_default
```


### 6c)

You will only use the default `caret` tuning grid in this assignment. We could customize it to see if the performance could be improved, but for now the default grid is all we will use.  

**What do the two tuning parameters in the `nnet` package correspond to?**  

#### SOLUTION

The two tuning parameters from `nnet` are `size` and `decay`. `size` corresponds
to the number of units in the hidden layer, and `decay` is a coefficient that tunes
the penalty associated with weight magnitudes (analogous to `lambda` in linear models).