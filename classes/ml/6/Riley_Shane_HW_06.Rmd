---
title: "ml Homework: 06"
subtitle: "Assigned February 17, 2022; Due: February 24, 2022"
author: "Shane Riley"
date: "Submission time: February 24, 2022 at 11:00PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators



## Overview

This assignment works through the details estimating an unknown mean, $\mu$, and unknown noise, $\sigma$, for a Gaussian likelihood. You will practice visualizing the log-posterior, work through the mathematics of the estimation process, and ultimately use the Laplace Approximation to approximate the joint posterior distribution on $\mu$ and $\sigma$ given observations. This assignment include programming and derivations.  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

## Load packages

You will use the `tidyverse` in this assignment, as you have done in the previous assignments.  

```{r, load_packages}
library(tidyverse)
```

## Problem 01

A large toy company recently completed a "digital transformation" and now collects, tracks, and records data from all areas involved in the production of their top selling toys. The company is interested in understanding the behavior of the plastic used in several toy lines and asks you to examine the data. After a few meetings with the company, you find out the performance metric they are interested in learning more about requires destructive tests. Thus, toys must be willingly destroyed in order to record the value of interest.  

Conducting the destructive tests is a tedious task and so only a small number of entries are available in the newly commissioned data warehouse that the company uses to store a majority of their data. You query the appropriate data tables in the data warehouse and return the following data set.  

```{r, read_in_data}
hw06_data_url <- "https://redacted"
hw06_df <- readr::read_csv(hw06_data_url, col_names = TRUE)
```

As you can see from the `glimpse()` below, `hw06_df` contains two columns. The `obs_id` column which is an observation index, and `x`, the performance metric of interest. There are just 8 observations to work with!  

```{r, show_data_glimpse}
hw06_df %>% glimpse()
```

It is believed that a Gaussian likelihood is appropriate for this performance metric. You feel it is appropriate to assume that the observations are conditionally independent given an unknown constant mean, $\mu$, and unknown noise, $\sigma$. With the $n$-th observation denoted as $x_n$, the joint likelihood can be factored into the product of $N$ likelihoods:  

$$ 
p \left( \mathbf{x} \mid \mu, \sigma \right) = \prod_{n=1}^{N} \left( \mathrm{normal} \left( x_n \mid \mu, \sigma \right) \right)
$$

Your goal is to infer the unknown mean of the performance metric, $\mu$, as well as the unknown noise, $\sigma$, using the 8 measurements, $\mathbf{x}$.

### 1a)

Start out by calculating a few summary statistics about the measurements.  

**Calculate the sample average, the sample standard deviation, the min and max values of the `x` variable in the `hw06_df` data set.**  

#### SOLUTION

```{r, solution_01a}
### your code here
sprintf("Sample average: %.2f", mean(hw06_df$x))
sprintf("Sample SD: %.2f", sd(hw06_df$x))
sprintf("Sample min: %.2f", min(hw06_df$x))
sprintf("Sample max: %.2f", max(hw06_df$x))
```

### 1b)

With such a small data set you decide to ask several Subject Matter Experts (SMEs) from the toy company their opinions about the performance metric. You find out the they have worked with this particular plastic for quite some time. However, while going through the "digital transformation" they also recently installed several new components to the machines that produce the plastic material. They are still getting used to working with the new equipment and software, but feel confident about the behavior of their material.  

After a few more meetings, the SMEs believe a Gaussian prior on the unknown mean is appropriate. The prior distribution on the unknown mean, $\mu$, will have prior mean, $\mu_0$, and prior standard deviation, $\tau_0$. The prior on $\mu$ is therefore:  

$$ 
\mu \mid \mu_0, \tau_0 \sim \mathrm{normal} \left( \mu \mid \mu_0, \tau_0 \right)
$$

The SMEs feel there is approximately 95% probability the mean would be between values of 10 and 12. They believe that interval is a middle 95% prior uncertainty interval, and so the prior median is between 10 and 12.  

**Determine the values for the prior hyperparameters, $\mu_0$ and $\tau_0$, based on the information provided by the SMEs.**  

#### SOLUTION

Uncertainty intervals in the context of the prior follow this format, where $t_{0.975}$
is the number of standard deviations from the mean that the interval reaches.

$$
a,b = \mu_0 \pm t_{0.975} \tau_0
$$
Rearranging, we confirm the intuition that $\mu_0$ is in the center of the interval.

$$
\mu_0 = \frac{b - a}{2} \\
a = 10 \\
b = 12 \\
\mu_0 = 11
$$

We will employ the heuristic that $t_{0.975} \approx 2$, to gather the standard
deviation:

$$
a = \mu_0 - t_{0.975} \tau_0 \\
\tau_0 = \frac{\mu_0 - a}{t_{0.975}} = 0.5
$$

Thus, the hyperparameters are both determined from the confidence interval.

### 1c)

You decide to treat the joint prior on $\mu$ and $\sigma$ as independent, $p\left(\mu,\sigma\right)=p\left(\mu\right)\times p\left(\sigma\right)$. The prior on the noise is assumed to be an Exponential distribution with a prior rate of 0.5, $\lambda = 0.5$.  

The un-normalized posterior on the two unknowns, $\mu$ and $\sigma$, is therefore:  

$$ 
p \left( \mu, \sigma \mid \mathbf{x} \right) \propto \prod_{n=1}^{N} \left( \mathrm{normal} \left(x_n \mid \mu, \sigma \right) \right) \times \mathrm{normal}\left(\mu \mid \mu_0, \tau_0\right) \times \mathrm{Exp}\left(\sigma \mid \lambda=0.5\right)
$$

Since there are just 2 unknowns, you can visualize the log-posterior surface to understand the joint posterior distribution on the unknowns. To do so, you will need to define a function which calculates the log-posterior at specific values of the unknown parameters. As you can see from the un-normalized posterior expression above, other pieces of information are required to calculate the log-posterior. The observations and prior hyperparameters must also be provided to the same function as the unknown parameters.  

Thus, before defining the log-posterior function, you must create an `R` list which stores the measurements, the hyperparameters associated with the prior on $\mu$, $\mu_0$ and $\tau_0$, and the hyperparameters associated with the prior on $\sigma$, $\lambda$.  

**The list of required information is started for you below. You must complete the code chunk below by assigning the correct values to each of the named elements in the list. The names of the variables and the comments specify what you should fill in.**  

#### SOLUTION

```{r, solution_01c, eval=TRUE}
hw06_info <- list(
  xobs = hw06_df$x,### the meausrements
  mu_0 = 11,### mu_0 value
  tau_0 = 0.5,### tau_0 value
  sigma_rate = 0.5 ### rate (lambda) on sigma
)
```

### 1d)

You must now define a function which calculates the log-posterior on the unknown mean, $\mu$, and unknown noise, $\sigma$. The `my_logpost()` function is started for you in the code chunk below. The first arguments, `unknowns`, is a vector containing the unknown parameters we wish to learn. The second argument, `my_info`, is a list of the required information. The unknowns are extracted from the `unknowns` vector for you with the unknown mean assigned to the `lik_mu` variable and the unknown noise assigned to the `lik_sigma` variable.  

The `my_info` second argument is a generic name, but you should assign it is a list with the same variables contained in the `hw06_info` list you defined in the previous problem. Just use the `$` operator whenever you want to access a piece of information from the `my_info` list in the `my_logpost()` function. For example, to access the vector of observations within the `my_logpost()` function you just need to type `my_info$xobs`.  

**Complete the `my_logpost()` function. The variable names and comments describe what you are required to complete.**  

**You ARE allowed to use built in `R` functions for densities in this problem.**  

**Several test values for the `unknowns` input vector are provided for you to try out below.**  

#### SOLUTION

```{r, solution_01d, eval=TRUE}
my_logpost <- function(unknowns, my_info)
{
  # unpack the unknowns into separate variables
  lik_mu <- unknowns[1]
  lik_sigma <- unknowns[2]
  
  # calculate the log-likelihood
  log_lik <- sum(dnorm(x = my_info$xobs,
                       mean = lik_mu,
                       sd = lik_sigma,
                       log = TRUE))
  
  # calculate the log-prior on mu
  log_prior_mu <- dnorm(lik_mu, mean = my_info$mu_0, sd = my_info$tau_0, log = TRUE)
  
  # calculate the log-prior on sigma
  log_prior_sigma <- dexp(lik_sigma, rate = my_info$sigma_rate, log = TRUE)
  
  # return the (un-normalized) log-posterior
  log_lik + log_prior_mu + log_prior_sigma
}
```

Test out the function to check that it works as expected. Try a value of 13 for $\mu$ and a value of 5 for $\sigma$. If you programmed the `my_logpost()` function correctly, you should get a value of -34.32184 printed to the screen.  

```{r, solution_01d_b}
my_logpost(c(13, 5), hw06_info)
```

Test out the function to check that it works as expected. Try a value of 7 for $\mu$ and a value of 1.5 for $\sigma$. If you programmed the `my_logpost()` function correctly, you should get a value of -78.65353 printed to the screen.  

```{r, solution_01d_c}
my_logpost(c(7, 1.5), hw06_info)
```

### 1e)

You must define a grid of parameter values that will be applied to the `my_logpost()` function, in order to visualize the log-posterior surface. A simple way to create a full-factorial grid of combinations is with the `expand.grid()` function. The basic syntax of `expand.grid()` is shown in the example code chunk below for two variable `x1` and `x2`. The `x1` variable is a vector of just two values, `c(1, 2)`, and the variable `x2` is a vector of 3 values, `1:3`. As shown in the code chunk output printed to the screen, the `expand.grid()` function produces 6 combinations of these two variables. The variables are stored as columns. Their combinations correspond to a row within the generated object. The `expand.grid()` function takes care of the "book keeping" for us, to allow varying `x2` for all values of `x1`.  

```{r, introduce_expandgrid}
expand.grid(x1 = c(1, 2),
            x2 = 1:3,
            # extra arguments I like to set
            KEEP.OUT.ATTRS = FALSE,
            stringsAsFactors = FALSE) %>% 
  # convert to a tibble!
  as.data.frame() %>% tibble::as_tibble()
```

You will use the `expand.grid()` function to create a grid of combinations of `mu` and `sigma`. You should create your `mu` and `sigma` variables in `expand.grid()` with the `seq()` function. The from (lower bound) and the to (upper bound) arguments that you should follow are:

* The lower bound on `mu` should equal 3 prior standard deviations away from the prior mean.  
* The upper bound on `mu` should equal 3 prior standard deviations above the prior mean.  
* The lower bound on `sigma` should equal 1.  
* The upper bound on `sigma` should equal the 0.99 **prior** quantile (99th **prior** percentile).  

**Complete the two code chunks below. In the first code chunk, define the lower and upper bounds on `mu` and `sigma` following the bulleted instructions. Use those bounds to create the grid of parameter combinations in the second code chunk below. Set the `length.out` argument in the `seq()` function to be 251 for both the `mu` and `sigma` variables.**  

#### SOLUTION

Define the bounds on the two parameters:  

```{r, solution_01e, eval=TRUE}
mu_grid_lwr <- hw06_info$mu_0 - 3 * hw06_info$tau_0
mu_grid_upr <- hw06_info$mu_0 + 3 * hw06_info$tau_0

sigma_grid_lwr <- 1
sigma_grid_upr <- qexp(0.99, rate = hw06_info$sigma_rate)
```

Define the grid of parameter combinations.  

```{r, solution_01e_b, eval=TRUE}
param_grid <- expand.grid(mu = seq(mu_grid_lwr, mu_grid_upr, length.out = 251),
                          sigma = seq(sigma_grid_lwr, sigma_grid_upr, length.out = 251),
                          KEEP.OUT.ATTRS = FALSE, stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()
```

### 1f)

The `my_logpost()` function accepts a vector as the first input argument, `unknowns`. Thus, you cannot simply pass in the columns of the `param_grid` tibble into `my_logpost()`! To overcome this, you will define a "wrapper" function, which manages the call to the log-posterior function. The wrapper, `eval_logpost()`, is started for you in the first code chunk below. The arguments to `eval_logpost()` are setup to be rather general. The first and second arguments, `unknown_1` and `unknown_2`, are the first and second elements in the `unknowns` input vector to the `my_logpost()` function. In the current context, the first argument is `mu` and the second argument is `sigma`. The third argument is intended to be a function handle for a log-posterior function, thus `logpost_func` represents the `my_logpost` function. The fourth argument represents the required information to call that log-posterior function.  

This problem tests that you understand how to call a function, and how to input the arguments to that function.  

**Complete the code chunk below, such that the user supplied `logpost_func` function is called. The `unknown_1` and `unknown_2` arguments must be combined together as the first argument to `logpost_func()`. Set the `logpost_info` variable as the second argument to `logpost_func()`.**  

*HINT*: If you are confused by this setup, think through how you called the `my_logpost()` function to test that it worked properly in Problem 1d).  

**Check that you setup `eval_logpost()` correctly by using the same first test in Problem 1d). Try a value of 13 for $\mu$ and a value of 5 for $\sigma$.**  

#### SOLUTION

```{r, solution_01f, eval=TRUE}
eval_logpost <- function(unknown_1, unknown_2, logpost_func, logpost_info)
{
  ### Return the logpost
  logpost_func(c(unknown_1, unknown_2), logpost_info)
}
```

Test out `eval_logpost()`. You should get the same as result that you did in Problem 1d). Remember the third argument to `eval_logpost()` is the log-posterior function we want to call.  

```{r, solution_01f_b}
### Compare
my_logpost(c(13, 5), hw06_info)
eval_logpost(13, 5, my_logpost, hw06_info)
```

### 1g)

The code chunk below uses the `purrr::map2_dfr()` function to apply the `eval_logpost()` function to all combinations of `mu` and `sigma` within `param_grid`. Be sure to set the `eval` flag to `TRUE` after you run the code chunk, because by default `eval=FALSE`. The result is assigned to the variable `log_post_result`. You can check the RStudio Environment Panel to see that the length of `log_post_result` is equal to the number of rows in `param_grid`.  

```{r, apply_over_grid, eval=TRUE}
log_post_result <- purrr::map2_dbl(param_grid$mu, param_grid$sigma,
                                   eval_logpost,
                                   logpost_func = my_logpost,
                                   logpost_info = hw06_info)
```

The code chunk below visualizes the log-posterior surface for you. The log-posterior surface contours are plotted in the same style presented in lecture. You are required to interpret the log-posterior surface, and include the sample average and sample standard deviation with a `geom_point()` geom object. The sample average and sample standard deviation will be displayed as an orange square marker within the figure. You will discuss how the posterior mode compares to these estimates.  

**The code chunk below is almost complete. You must assign the sample average to the `xbar` variable and the sample standard deviation to the `xsd` variable in the `tibble` assigned as the `data` argument to the `geom_point()` geom. See the comments below for where you should make the changes.**  
**You must describe how the sample average and standard deviation compare to the posterior mode. Are they similar? What can you say about the posterior uncertainty in $\mu$ and $\sigma$ based on the visualization?**  

*HINT*: If you want to see what the log-posterior surface looks like before adding in the sample average and sample standard deviation point, just comment out all lines associated with the `geom_point()` call below.  

#### SOLUTION

What do you think?  

```{r, solution_01g, eval=TRUE}
param_grid %>% 
  mutate(log_post = log_post_result,
         log_post_2 = log_post - max(log_post)) %>% 
  ggplot(mapping = aes(x = mu, y = sigma)) +
  geom_raster(mapping = aes(fill = log_post_2)) +
  stat_contour(mapping = aes(z = log_post_2),
               breaks = log(c(0.01/100, 0.01, 0.1, 0.5, 0.9)),
               size = 2.2,
               color = "black") +
  # include the sample average (xbar) and the sample standard deviation (xsd)
  geom_point(data = tibble::tibble(xbar = mean(hw06_df$x), xsd = sd(hw06_df$x)),
             mapping = aes(x = xbar, y = xsd),
             shape = 22,
             size = 4.5, fill = "orange", color = "steelblue") +
  scale_fill_viridis_c(guide = FALSE, option = "viridis",
                       limits = log(c(0.01/100, 1.0))) +
  labs(x = expression(mu), y = expression(sigma)) +
  theme_bw()
```

The posterior distribution as related to the prior and the sample data is as expected. 
Since the number of data points is small, the mean a-posteriori estimate (MAP) is
much closer to the prior mean and prior standard deviation than to the sample mean
and standard deviation. With that said, the prior distributions are successful in
"educating" the model and tightening the confidence intervals on $\mu$ and $\sigma$,
even with so few data points.


## Problem 02

We discussed in lecture how the visualization approach is useful, but is limited to just 1 or 2 unknowns. It does not scale well to more unknowns. We discussed that the Laplace or Normal Approximation allows us to approximate a distribution with a Multivariate Normal (MVN) distribution. The Laplace Approximation is convenient and useful for performing Bayesian inference in a wide variety of problems. You will ultimately perform the Laplace Approximation on the problem described in Problem 01.  

The Laplace Approximation consists of three main steps. The first step finds the posterior mode via optimization, the second step evaluates the Hessian matrix at the posterior mode, and the third step calculates the approximate covariance matrix from the Hessian. You practiced the first step, finding the posterior mode, in the previous assignment with the one-parameter normal-normal model. Let's complete the Laplace Approximation for the one-parameter problem before executing the Laplace Approximation for the two parameter case. This gives you experience with each of the steps in the Laplace Approximation in a simplified setting, before applying the approximation to the more challenging two unknowns problem.  

You will assume that the likelihood noise is equal to 3, $\sigma = 3$. All observations are still considered to be conditionally independent given the $\mu$ and $\sigma$ parameters. The prior on the unknown mean is still a Gaussian with hyperparmeters $\mu_0$ and $\tau_0$. The un-normalized posterior on the unknown mean given $N$ observations, $\mathbf{x}$, and likelihood noise, $\sigma$, is:  

$$ 
p\left( \mu \mid \mathbf{x}, \sigma \right) \propto \prod_{n=1}^{N} \left( \mathrm{normal} \left( x_n \mid \mu, \sigma \right) \right) \times \mathrm{normal}\left(\mu \mid \mu_0, \tau_0\right)
$$

### 2a)

You wrote out the un-normalized log-posterior on $\mu$, determined the first derivative with respect to $\mu$, and derived the posterior mode (the MAP) in Problem 02 of Homework 04. Thus, you already performed the first step of the Laplace Approximation! You will work through the details of the second and third steps, starting with calculating the second derivative of the log-posterior with respect to the unknown mean, $\mu$.  

**Determine the second derivative of the log-posterior with respect to the unknown mean, $\mu$. Your solution should show at least several steps. You may reference your solution from the previous assignment, but you must write down the expression you are using as your starting point.**  

#### SOLUTION

First, the expression for the first derivative of the un-normalized log-posterior
is recalled:

$$
\frac{1}{\sigma^2}\sum_{n=1}^N \left(x_n - \mu \right) 
- \frac{1}{\tau_0^2} \left(\mu - \mu_0 \right)
$$

Next, a derivative with respect to $\mu$ is applied again to obtain the second
derivative:

$$
\frac{\mathrm{d}}{\mathrm{d}\mu} \left( \frac{1}{\sigma^2}\sum_{n=1}^N \left(x_n - \mu \right) 
- \frac{1}{\tau_0^2} \left(\mu - \mu_0 \right) \right) \\
\to - \left( \frac{N}{\sigma^2} + \frac{1}{\tau_0^2} \right) \\
\to - \left( \frac{N \tau_0^2 + \sigma^2}{\sigma^2 \tau_0^2}\right)
$$

### 2b)

You determined the expression for the posterior mode in Problem 2d) of Homework 04.  

**How can you confirm that the mode does in fact correspond to the $\mu$ value associated with the maximum log-posterior density and not the minimum log-posterior density?**  

#### SOLUTION

Stationary points (points where the gradient is equal to 0) can be identified as
local maxima or minima by examining the curvature (second derivative) at the
stationary point. In the 2-dimensional context, this is the sign of the second
derivative. Since the second derivative of the un-normalize log posterior on
$\mu$ in this case is always negative, it is known that the stationary point
identified is indeed associated with the maximum log-posterior density.

### 2c)

In this one parameter application, the Laplace Approximation approximates the posterior distribution as an univariate Gaussian.  

$$ 
p\left( \mu \mid \mathbf{x}, \sigma \right) \approx \mathrm{normal} \left( \mu \mid \mathrm{m}_N, \mathrm{s}_N \right)
$$

where $\mathrm{m}_N$ is the Laplace Approximation posterior mean and $\mathrm{s}_N$ is the Laplace Appoximation posterior standard deviation. Since this is a single parameter setting, the covariance matrix is just a scalar value (the variance). The square root of the variance is the standard deviation. You must determine the approximate posterior standard deviation using your result for the second derivative in Problem 2a).  

**Write out the expressions for the approximate posterior mean and posterior standard deviation. You may use the expression for the posterior mode from the previous assignment. You may write the posterior standard deviation in terms of precision.**  

#### SOLUTION

The MAP is recalled:

$$
\mu_{MAP} = \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{N}{\sigma^2} \bar{x}}{\frac{1}{\tau_0^2} + \frac{N}{\sigma^2}}
$$

Since the mode is always equal to the mean on a Gaussian distribution, our approximate
posterior mean will be equal to the MAP. The approximate posterior standard deviation
is equal to our given standard deviation ($\sigma$ is known).

$$
m_N = \mu_{MAP} = \frac{\tau_0^{2}N\bar{x} + \sigma^{2} \mu_0}{\sigma^{2}+\tau_0^{2}N}\\
s_N = -\left( \frac{\mathrm{d}^2}{\mathrm{d} \mu^2} \log \left( p\left( \mu \mid \mathbf{x}, \sigma \right)\right)\right)^{-1} = \left( \frac{\sigma^2 \tau_0^2}{N \tau_0^2 + \sigma^2}\right)
$$

It is worth noting in this case:

$$
\mu_N = m_N \\
\tau_N = s_N
$$
This means that the Laplace Approximation provides an exact representation of
the posterior distribution.

### 2d)

We saw in lecture how the Laplace Approximation is just that, an approximation. However, for this specific application (one parameter normal-normal model with an unknown mean) the Laplace Approximation is **not** an approximation. In fact, the expressions for the posterior mean and posterior precision were discussed in lecture.  

**Why is the Laplace Approximation equal to the exact posterior distribution for this specific application?**  

#### SOLUTION

The approximation is equal to the posterior distribution, because both are simple
Gaussian distributions. The posterior is Gaussian in turn because the likelihood
and prior are both Gaussian (conjugate prior). Since we are applying a Laplace 
approximation to a normal distribution, we are able to describe it perfectly.

## Problem 03

Let's now return to the two parameter application from Problem 01 with the goal of learning the unknown mean, $\mu$, and unknown noise, $\sigma$. However, before applying the Laplace Approximation to this setting, you will perform a change-of-variables transformation to $\sigma$. The transformed variable, $\varphi$, is related to $\sigma$ through the transformation or link function $g\left(\cdot\right)$:  

$$ 
\varphi = g\left( \sigma \right)
$$

### 3a)

**Why is it useful to transform $\sigma$ to $\varphi$ using a transformation like the natural log when we perform the inference with the Laplace Approximation?**  

#### SOLUTION

It is useful because we can guarantee positive values for $\sigma$ by taking a logarithm,
and thus respect the natural bounds of that variable while using a naturally-unbounded
gaussian curve to approximate.

### 3b)

The generic inverse link function back-transforms from $\varphi$ to the noise, $\sigma$:  

$$ 
\sigma = g^{-1} \left( \varphi \right)
$$

**Write out the un-normalized joint posterior between the unknown mean, $\mu$, and the transformed noise, $\varphi$, via the probability change-of-variables formula.**  

**You do NOT need to simplify the distributions in any way. You may write the "names" or labels of the distributions (such as $\mathrm{normal}()$ and $\mathrm{Exp}()$). You must correctly substitute in for the inverse link function into the log-posterior "based" on the original parameter $\sigma$. You must include all terms from the change-of-variables formula.**  

#### SOLUTION

The un-normalized join posterior is expressed:

$$
p \left( \mu, \sigma \mid \mathbf{x} \right) = \prod_{n=1}^N \left( p \left( x_n \mid \mu,\sigma  \right) \right)
\times p \left( \mu \mid \mu_0, \tau_0 \right) 
\times p \left( \sigma \mid \lambda\right)\\
= \prod_{n=1}^N \mathrm{normal} \left(  x_n \mid \mu, \sigma \right) 
\times \mathrm{normal} \left( \mu \mid \mu_0, \tau_0 \right)
\times \mathrm{Exp} \left( \sigma \mid \lambda  \right)
$$

Transforming into $\varphi$ space:

$$
\prod_{n=1}^N \mathrm{normal} \left(  x_n \mid \mu, g^{-1} \left( \varphi \right) \right) 
\times \mathrm{normal} \left( \mu \mid \mu_0, \tau_0 \right)
\times \mathrm{Exp} \left( g^{-1} \left( \varphi \right) \mid \lambda  \right)
\times \left| \frac{\mathrm{d}}{\mathrm{d} \varphi} \left( g^{-1} \left( \varphi \right)  \right) \right|
$$

Finally, the logarithm is applied:
$$
\log \left( \prod_{n=1}^N \mathrm{normal} \left(  x_n \mid \mu, g^{-1} \left( \varphi \right) \right) 
\times \mathrm{normal} \left( \mu \mid \mu_0, \tau_0 \right)
\times \mathrm{Exp} \left( g^{-1} \left( \varphi \right) \mid \lambda  \right)
\times \left| \frac{\mathrm{d}}{\mathrm{d} \varphi} \left( g^{-1} \left( \varphi \right)  \right) \right| \right) \\
= \sum_{n=1}^N \log \left( \mathrm{normal} \left(  x_n \mid \mu, g^{-1} \left( \varphi \right) \right) \right)
+ \log \left( \mathrm{normal} \left( \mu \mid \mu_0, \tau_0 \right) \right)
+ \log \left( \mathrm{Exp} \left( g^{-1} \left( \varphi \right) \mid \lambda  \right) \right)
+ \log \left| \frac{\mathrm{d}}{\mathrm{d} \varphi} \left( g^{-1} \left( \varphi \right)  \right) \right|
$$


### 3c)

The weight example in lecture used the logit function as the transformation function. You will not use the logit function. Instead, you will use the natural log as the link function:  

$$ 
\varphi = g\left( \sigma \right) = \log \left( \sigma \right)
$$

**Write out the inverse link function and derive the natural log of the derivative adjustment.**  

#### SOLUTION

The inverse link function is described:

$$
g^{-1}\left(\varphi \right) = \sigma = \exp \left( \varphi \right)
$$

The log of the derivative adjustment is described:

$$
\log \left| \frac{\mathrm{d}}{\mathrm{d}\varphi} g^{-1} \left( \varphi \right) \right| \\
= \log \left| \exp \left( \varphi \right)\right| \\
= \varphi
$$

### 3d)

You must now define a function to calculate the log-posterior between $\mu$ and $\varphi$. The `my_cv_logpost()` is started for you in the code chunk below. It also uses two input arguments, with the same names as the `my_logpost()` function. The first argument is again the vector of unknowns and the second argument is the list of required information. However, the `unknowns` vector is intended to be different from that in `my_logpost()`. As shown in the code chunk below, the second element of `unknowns` corresponds to the transformed noise parameter, $\varphi$.  

Note that you will use the same list of required information, `hw06_info`, that you defined in previously in Problem 01.  

**Complete the `my_cv_logpost()` function. The variable names and comments describe what you are required to complete.**  

**You ARE allowed to use built in `R` functions for densities in this problem.**  

**Several test values for the `unknowns` input vector are provided for you to try out below.**  

#### SOLUTION

```{r, solution_03d, eval=TRUE}
my_cv_logpost <- function(unknowns, my_info)
{
  # unpack the unknowns into separate variables
  lik_mu <- unknowns[1]
  lik_varphi <- unknowns[2]
  
  # back transform to sigma
  lik_sigma <- exp(lik_varphi)
  
  # calculate the log-likelihood
  log_lik <- sum(dnorm(x = my_info$xobs,
                       mean = lik_mu,
                       sd = lik_sigma,
                       log = TRUE))
  
  # calculate the log-prior on mu
  log_prior_mu <- dnorm(lik_mu, mean = my_info$mu_0, sd = my_info$tau_0, log=TRUE)
  
  # calculate the log-prior on sigma3
  log_prior_sigma <- dexp(x = lik_sigma, rate = my_info$sigma_rate, log = TRUE)
  
  # calculate the log-derivative adjustment
  log_deriv_adjust <- lik_varphi
  
  # return the (un-normalized) log-posterior
    log_lik + log_prior_mu + log_prior_sigma + log_deriv_adjust
  
}
```

Test out the function to check that it works as expected. Try a value of 13 for $\mu$ and a value of 0 for $\varphi$. If you programmed the `my_logpost()` function correctly, you should get a value of -83.66777 printed to the screen.  

```{r, solution_03d_b}
###
my_cv_logpost(c(13, 0), hw06_info)
```

Test out the function to check that it works as expected. Try a value of 7 for $\mu$ and a value of -1 for $\varphi$. If you programmed the `my_logpost()` function correctly, you should get a value of -605.1904 printed to the screen.  

```{r, solution_03d_c}
###
my_cv_logpost(c(7, -1), hw06_info)
```

### 3e)

Let's visualize what the the log-posterior surface looks like in the $\mu$, $\varphi$ space. You must define a grid of parameter combinations, similarly to what you did in Problem 01. However, this time you must define the grid in terms of combinations of $\mu$ and $\varphi$ (instead of $\mu$ and $\sigma$).  

**You must define the from (lower bound) and to (upper bounds) on the $\varphi$ parameter. You should apply the natural log link function to the bounds on $\sigma$ defined in Problem 1e). After specifying the bounds, create the full-factorial combinations between `mu` and `varphi` using the `expand.grid()` function. Use the same bounds on `mu` that you used in Problem 01 and use `length.out=251` for both parameters. Assign the result to the `cv_param_grid` variable.**  

#### SOLUTION

Define the bounds on $\varphi$ for the grid.  

```{r, solution_03e_a, eval=TRUE}
varphi_grid_lwr <- log(sigma_grid_lwr)
varphi_grid_upr <- log(sigma_grid_upr)
```

Create the grid of full-factorial combinations with $\mu$.  

```{r, solution_03e_b, eval=TRUE}
cv_param_grid <- expand.grid(mu = seq(mu_grid_lwr, mu_grid_upr, length.out = 251), 
                             varphi = seq(varphi_grid_lwr, varphi_grid_upr, length.out = 251), 
                             KEEP.OUT.ATTRS = FALSE, stringsAsFactors = FALSE) %>%  
  as.data.frame() %>% tibble::as_tibble()
```

### 3f)

The `eval_logpost()` function was defined using generic variable names in order to be used for the original log-posterior evaluation **and** the change-of-variables log-posterior. Problem 1g) demonstrated how to apply `eval_logpost()` to every combination of `mu` and `sigma` using the `purrr::map2_dbl()` function. You should follow those steps but adapt the code provided to you in Problem 1g) in order to calculate the `my_cv_logpost()` function to every combination of `mu` and `varphi` contained in `cv_param_grid`.  

**Apply the `eval_logpost()` function to every combination of variables in `cv_param_grid`. You must use the `purrr::map2_dbl()` function to functionally loop over all combinations in `cv_param_grid`. You may follow the code example provided in Problem 1g). However, be careful to change the variable names! Assign the result to the `log_post_cv_results`.**  

```{r, solution_03f, eval=TRUE}
log_post_cv_result <- purrr::map2_dbl(cv_param_grid$mu, cv_param_grid$varphi,
                                      eval_logpost,
                                      logpost_func = my_cv_logpost,
                                      logpost_info = hw06_info)
```


### 3g)

The log-posterior surface between $\mu$ and $\varphi$ is visualized for you in the code chunk below. As in Problem 1g), you must complete the `geom_point()` by including the sample average and the log-transformed sample standard deviation.  

**Complete the `geom_point()` call in the code chunk below. The comments specify where you should include the sample average and the log of the sample standard deviation. Describe the contour shapes of the log-posterior and how the overall shape compares to the log-posterior in the original parameter space between $\mu$ and $\sigma$.**  

#### SOLUTION

```{r, solution_03g, eval=TRUE}
cv_param_grid %>% 
  mutate(log_post = log_post_cv_result,
         log_post_2 = log_post - max(log_post)) %>% 
  ggplot(mapping = aes(x = mu, y = varphi)) +
  geom_raster(mapping = aes(fill = log_post_2)) +
  stat_contour(mapping = aes(z = log_post_2),
               breaks = log(c(0.01/100, 0.01, 0.1, 0.5, 0.9)),
               size = 2.2,
               color = "black") +
  # include the sample average (xbar) and the log-sample standard deviation (log_xsd)
  geom_point(data = tibble::tibble(xbar = mean(hw06_df$x), log_xsd = log(sd(hw06_df$x))),
             mapping = aes(x = xbar, y = log_xsd),
             shape = 22,
             size = 4.5, fill = "orange", color = "steelblue") +
  scale_fill_viridis_c(guide = FALSE, option = "viridis",
                       limits = log(c(0.01/100, 1.0))) +
  labs(x = expression(mu), y = expression(varphi)) +
  theme_bw()
```

The relative locations of the sample point and the MAP are similar between the 
two plots. However, the plot that includes the link function is much more even
and elliptical than without. Because it is more elliptical, it should be easier
to produce an accurate Laplace approximation to this distribution.

## Problem 04

It's now time to perform the Laplace Approximation on your transformed two parameter model. The first step is to find the posterior mode. You will not calculate the gradient vector and perform the optimization by hand in this question. Instead, you will use the `optim()` function to perform the optimization.  

### 4a)

The code chunk below defines two different initial guesses for the unknown mean, $\mu$, and unknown log-transformed noise, $\varphi$. You will try out both initial guesses and compare the optimization results.  

```{r, define_opt_init_guess}
init_guess_01 <- c(10, 0.75)

init_guess_02 <- c(11.75, 1.85)
```

Let's first visualize these two points relative to the posterior mode, since you know what the log-posterior surface looks like.  

**Complete the code chunk below by visualizing the two different initial guesses with a `geom_point()` geom on top of the log-posterior surface. Think through which element in the `init_guess_01` and `init_guess_02` vectors corresponds to which parameter. You do not need to change the `aes()` call within the `geom_point()` geom below. You must correctly specify the variables in the `tibble` of the `data` argument to `geom_point()`.**  

#### SOLUTION

```{r, solution_04a, eval=TRUE}
cv_param_grid %>% 
  mutate(log_post = log_post_cv_result,
         log_post_2 = log_post - max(log_post)) %>% 
  ggplot(mapping = aes(x = mu, y = varphi)) +
  geom_raster(mapping = aes(fill = log_post_2)) +
  stat_contour(mapping = aes(z = log_post_2),
               breaks = log(c(0.01/100, 0.01, 0.1, 0.5, 0.9)),
               size = 2.2,
               color = "black") +
  # include the initial guess points
  geom_point(data = tibble::tibble(attempt = as.character(1:2),
                                   mu = c(init_guess_01[1], init_guess_02[1]),
                                   varphi = c(init_guess_01[2], init_guess_02[2])),
             mapping = aes(color = attempt),
             size = 4.5) +
  scale_fill_viridis_c(guide = FALSE, option = "viridis",
                       limits = log(c(0.01/100, 1.0))) +
  labs(x = expression(mu), y = expression(varphi)) +
  theme_bw()
```

### 4b)

You will now find the posterior mode (the MAP) on the $\mu$ and $\varphi$ parameters. You will repeat the optimization process twice. The first will use the `init_guess_01` starting guess, and the second will use the `init_guess_02` starting guess. Make sure you use the log-posterior function associated with the transformed noise.  

**Complete the two code chunks below. The first code chunk finds the posterior mode (MAP) based on the first initial guess `init_guess_01` and the second code chunk uses the second initial guess `init_guess_02`. You must fill in the arguments to the `optim()` call to find the $\mu$ and $\varphi$ values which maximize the `my_cv_logpost()` function.**  

**To receive full credit you must:**  
* **specify the initial guesses correctly**  
* **specify the function to be optimized**  
* **specify the gradient evaluation correctly**  
* **correctly pass in the list of required information**  
* **specify the `"BFGS"` algorithm to be used**  
* **instruct `optim()` to return the Hessian matrix**  
* **make sure `optim()` maximizes the log-posterior instead of trying to minimize it**  
* **the max iterations (`maxit`) to be 1001**  

#### SOLUTION

Use the first initial guess.  

```{r, solution_04b_a, eval=TRUE}
map_res_01 <- optim(init_guess_01,
                    my_cv_logpost,
                    gr = NULL,
                    hw06_info,
                    method = "BFGS",
                    hessian = TRUE,
                    control = list(fnscale = -1, maxit = 1001)
)
map_res_01
```

Use the second initial guess.  

```{r, solution_04b_b, eval=TRUE}
map_res_02 <- optim(init_guess_02,
                    my_cv_logpost,
                    gr = NULL,
                    hw06_info,
                    method = "BFGS",
                    hessian = TRUE,
                    control = list(fnscale = -1, maxit = 1001)
)
map_res_02
```

### 4c)

You tried two different starting guesses...are the optimization results different?  

**Are the identified optimal parameter values the same? Are the Hessian matrices the same? Was anything different between the optimizations?**  

**What about the log-posterior surface gave you a hint about how the two results would compare?**  


#### SOLUTION

The identified optimal parameters and Hessian matrices are about the same, to four
decimal places. This is expected, as the log posterior has only one maximum. However,
the second initial guess takes almost twice as many iteration to converge to the solution.

The contour lines show that guess 1 lies on a steeper surface than guess 2. As such,
it makes sense that guess 2 would have a harder time navigating its portion of the
curve.

### 4d)

Finding the posterior mode is the first step in the Laplace Approximation. The second step uses the negative inverse of the Hessian matrix as the approximate posterior covariance matrix. You wil use a function, `my_laplace()`, to perform the complete Laplace Approximation. This one function is all that is needed to perform all steps of the Laplace Approximation.  

**Complete the code chunk below. The `my_laplace()` function is adapted from the `laplace()` function from the `LearnBayes` package. Fill in the missing pieces to double check that you understand which portions of the optimization result correspond to the mode and which are used to approximate the posterior covariance matrix.**  

#### SOLUTION

Complete the missing pieces of the code chunk below. The last portion of the `my_laplace()` function compiles the results into a list.  

```{r, solution_04d, eval=TRUE}
my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian =  TRUE,
               control = list(fnscale = -1, maxit = 5001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian) 
  p <- length(mode)
  # we will discuss what int means in a few weeks...
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```

### 4e)

You will now perform the Laplace Approximation to determine the approximate posterior on the $\mu$ and $\varphi$ parameters given the measurements.  

**Call the `my_laplace()` function to approximate the posterior on $\mu$ and $\varphi$. Check that solution converged. Display the posterior means on each parameter. Display the posterior standard deviations on each parameter. What is the posterior correlation coefficient between $\mu$ and $\varphi$?**  

#### SOLUTION

Execute the Laplace Approximation.  

```{r, solution_04e, eval=TRUE}
laplace_result <- my_laplace(init_guess_01, my_cv_logpost, hw06_info)
mean_mu <- laplace_result$mode[1]
mean_varphi <- laplace_result$mode[2]
sd_mu <- sqrt(laplace_result$var_matrix[1,1])
sd_varphi <- sqrt(laplace_result$var_matrix[2,2])
correlation_coeff <- laplace_result$var_matrix[2,2] / (sd_mu * sd_varphi)

laplace_result <- my_laplace(init_guess_01, my_cv_logpost, hw06_info)
sprintf("Posterior mean mu: %.3f", mean_mu)
sprintf("Posterior mean varphi: %.3f", mean_varphi)
sprintf("Posterior mean sigma: %.3f", exp(mean_varphi))
sprintf("Posterior sd mu: %.3f", sd_mu)
sprintf("Posterior sd varphi: %.3f", sd_varphi)
sprintf("Posterior correlation coeff: %.3f", correlation_coeff)
```

## Problem 05

You will now use the Laplace Approximation to answer questions from the toy company described back in Problem 01. After all, we were learning the unknown parameters to describe behavior. It is now time to discuss what you learned!  

### 5a)

**Use the Laplace Approximation result to calculate the probability that the unknown mean, $\mu$, is less than the sample average.**  

#### SOLUTION

```{r, solution_05a}
pnorm(mean(hw06_info$xobs), mean=mean_mu, sd=sd_mu)
```

The chance of the true mean (in light of the prior without a selection of $\sigma$)
being below $\bar{x}$ is 8.05%.

### 5b)

The Laplace Approximation result in Problem 04 is between the $\mu$ and $\varphi$ parameters. However, the toy company described in Problem 01 is not interested in the $\varphi$ parameter. They want to know about the noise in their process, and thus are interested in $\sigma$ not $\varphi$. You will need to undo the change-of-variables transformation, while accounting for any potential posterior correlation with $\mu$.  

Rather than working through the math to accomplish this, let's just use random sampling. The Laplace Approximation is a known distribution type, specifically a MVN distribution. You will call a MVN random number generator, `MASS::mvrnom()`, to generate random observations from a MVN with a user specified mean, `mu`, and user specified covariance matrix, `Sigma`. You will then back-transform from $\varphi$ to $\sigma$ by simply calling the inverse link function.  

The `generate_post_samples()` function is started for you in the code chunk below. The user provides the Laplace Approximation result as the first argument, `mvn_info`, and the number of samples to generate, `num_samples`. The `MASS::mvrnorm()` function is used to generate the posterior samples. A few data conversion steps are made before piping the result to a `mutate()` call. You **must** apply the correct inverse link function to calculate $\sigma$ based on the randomly generated values of $\varphi$.  

**Complete the code chunk below. Assign the correct arguments to the `mu` and `Sigma` arguments to `MASS::mvrnorm()`. Use the correct inverse link function to back-transform from $\varphi$ to $\sigma$.**  

*NOTE*: The `MASS` package is installed with base `R`, so you do **NOT** need to download it.  

#### SOLUTION

```{r, solution_05b, eval=TRUE}
generate_post_samples <- function(mvn_info, num_samples)
{
  MASS::mvrnorm(n = num_samples,
                mu = mvn_info$mode,
                Sigma = mvn_info$var_matrix) %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    purrr::set_names(c("mu", "varphi")) %>% 
    mutate(sigma = exp(varphi))
}
```


### 5c)

**Generate 1e4 posterior samples from the Laplace Approximation posterior distribution and assign the result to the variable `post_samples`.**  

**Use the `summary()` function to quickly summarize the posterior samples on each of the parameters.**  

**Apply the correct inverse link function to the posterior mean on `varphi`. Does the result equal the posterior mean on `sigma`?**  

#### SOLUTION

```{r, solution_05c, eval=TRUE}
set.seed(202004)
post_samples <- generate_post_samples(laplace_result, 1e4)
post_samples %>% summary()
```

The mean value for standard deviation from the generated points is close to, but
not quite, the standard deviation from the Laplace approximation. The histogram
that comes later provides an explanation of this. Since values of sigma cannot exist
below 0, the actual trend is actually not Gaussian, meaning that the mode is not
necessary the mean any longer.

### 5d)

**Use `ggplot2` to visualize the posterior histograms on the $\mu$ and $\sigma$ parameters. Set the number of bins to 55. You may use separate `ggplot()` calls for each histogram.**  

**Does the posterior distribution on $\sigma$ look Gaussian?**  

#### SOLUTION

```{r, solution_05d, eval=TRUE}
post_samples %>% ggplot(mapping = aes(x = mu)) +
  geom_histogram(bins = 55, color = "navyblue", fill = "gold", size = 1.55)
```

```{r, solution_05d2, eval=TRUE}
post_samples %>% ggplot(mapping = aes(x = sigma)) +
  geom_histogram(bins = 55, color = "navyblue", fill = "gold", size = 1.55)
```

### 5e)

The toy company would like to know based on the limited data set the variation in their manufacturing process. Specifically, they want to know the probability that the noise is greater than 4 units.  

**Calculate the posterior probability that $\sigma$ is greater than 4.**  

*HINT*: Remember the basic definition of probability!  

#### SOLUTION

To use the randomly-sampled data, we simply calculate the fraction of values with
$\sigma$ in excess of 4:

```{r, solution_05e}
mean(post_samples$sigma > 4)
```

The chance that $\sigma > 4$ is 10.8%